---
title: week_six = logistic regression.
bigimg: /img/wk-6.jpeg
comments: true
tags: [statistical analysis,logistic regression]
---

This week's work comes after attending the Data science workshop in Nyeri.
It's taken a while to log for this week.
Focus for this week is on;

# Logistic Regression with Python


## Import Libraries

Let's import some libraries to get started!


```jupyter
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set_style('whitegrid')
```

## The Data

We will be working with the [Titanic Data Set from Kaggle](https://www.kaggle.com/c/titanic) downloaded as titanic_train.csv file


```jupyter
train = pd.read_csv('titanic_train.csv')

train.head(2)
```




<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
  </tbody>
</table>
</div>



# Exploratory Data Analysis

Some exploratory data analysis!

We'll start by checking out missing data!

## Missing Data

We can use seaborn to create a simple heatmap to see where we are missing data!


```jupyter
sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')

# An assessment of data available, Age and Cabin have missing values while the rest
# are relatively OK.

```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a208d1908>




![png](/notebooks/logistic_regression/output_5_1.png)


#### Visualizing some more of the data

analysis by column. By Survival


```jupyter
sns.countplot(x='Survived',data=train)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a20274080>




![png](/notebooks/logistic_regression/output_7_1.png)


#### Survival by Gender


```jupyter
sns.countplot(x='Survived',hue='Sex',data=train,palette='RdBu_r')
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a20d895c0>




![png](/notebooks/logistic_regression/output_9_1.png)


#### Survival by Passenger Class


```jupyter
sns.countplot(x='Survived',hue='Pclass',data=train)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a20976320>




![png](/notebooks/logistic_regression/output_11_1.png)


#### Distribution of Passengers on board by Age


```jupyter
sns.distplot(train['Age'].dropna(),kde=False,bins=30)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a20e2ac18>




![png](/notebooks/logistic_regression/output_13_1.png)


#### Passengers onboard with sibling(s) / spouse


```jupyter
sns.countplot(x='SibSp',data=train)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a20ede5c0>




![png](/notebooks/logistic_regression/output_15_1.png)


#### Passengers by amount of fare paid


```jupyter
train['Fare'].hist(bins=20,figsize=(10,5))
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a210737b8>




![png](/notebooks/logistic_regression/output_17_1.png)


# Data Cleaning

#### Imputation.  
    Filling out missing values by approximation
Filling in the mean age to the age column

Start of by checking the average age by passenger class.


```jupyter
plt.figure(figsize=(10,7))
sns.boxplot(x='Pclass',y='Age',data=train)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a20ff03c8>




![png](/notebooks/logistic_regression/output_19_1.png)


Wealthier passengers in the higher classes tend to be older,

We'll use these average age values to impute missing data based on Pclass for Age.


```jupyter
def impute_age(cols):
    Age = cols[0]
    Pclass = cols[1]

    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24

    else:
        return Age

```

Apply `impute_age` function


```jupyter
train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)

```

And by checking for missing values on our data, we have;


```jupyter
sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')

```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a212b75f8>




![png](/notebooks/logistic_regression/output_25_1.png)


We can Drop the `Cabin` column as it possesses a huge percentage of missing values and filling
in may not be appropriatte.

Also we will drop the few instances on the `Embarked` column



```jupyter
train.drop('Cabin',axis=1,inplace=True)
```


```jupyter
train.head(2)
```




<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C</td>
    </tr>
  </tbody>
</table>
</div>




```jupyter
#drop missing row record from embarked column
train.dropna(inplace=True)
```

## Convert Categorical Features

We need to convert categorical features to dummy variables using pandas,

Otherwise the learning algorithm won't be able to directly take in those features as inputs.

For the sex column, caterorize if passenger is male or not(1 | 0 )

On embarkment point it will be Q, S 0r C.


```jupyter
sex = pd.get_dummies(train['Sex'],drop_first=True)

embark = pd.get_dummies(train['Embarked'],drop_first=True)
```

#### Concatenate the generated categorical columns to the dataset


```jupyter
train = pd.concat([train, sex,embark],axis=1)
train.head(2)
```




<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>male</th>
      <th>Q</th>
      <th>S</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>S</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



#### Select Columns that we will use for the model


```jupyter
train.drop(['Name','Sex','Embarked','Ticket','PassengerId'],axis=1,inplace=True)

```


```jupyter
train.head(2)
```




<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>male</th>
      <th>Q</th>
      <th>S</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



And the data is ready for our model!


# Building a Logistic Regression model

Start by splitting data into a training set and test set


## Train Test Split



    X = These are the features we will use to predict

    y = Value we are predicting ie Did the passenger survive


```jupyter
X = train.drop('Survived',axis=1)
y = train['Survived']
```


```jupyter
from sklearn.cross_validation import train_test_split
```


```jupyter
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)
```

## Training and Predicting


```jupyter
from sklearn.linear_model import LogisticRegression

```


```jupyter
# create an instance of LR model
logmodel = LogisticRegression()
```


```jupyter
# train the model

logmodel.fit(X_train,y_train)
```




    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
              verbose=0, warm_start=False)




```jupyter
# predict using the model
predictions = logmodel.predict(X_test)
```

## Evaluate the Model

Using classification report, We can check :
    - precision
    - recall
    - f1-score


```jupyter
from sklearn.metrics import classification_report,confusion_matrix
```


```jupyter
print(classification_report(y_test,predictions))
```

                 precision    recall  f1-score   support

              0       0.80      0.91      0.85       163
              1       0.82      0.65      0.73       104

    avg / total       0.81      0.81      0.80       267



A **confusion matrix** can also be applied

in order to determine how many observations were correctly or incorrectly classified.


```jupyter
confusion_matrix(y_test,predictions)
```




    array([[148,  15],
           [ 36,  68]])
